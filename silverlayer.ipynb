{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62e97cd3-fd08-45b4-8f1e-d566606b6f1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType, DecimalType, TimestampType, DateType\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ff82534-25b7-449f-b398-60c86398ce4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#ft_consumidores\n",
    "print(\"Processando silver.ft_consumidores...\")\n",
    "\n",
    "#ler a tabela bronze\n",
    "df_consumidores_bronze = spark.table(\"bronze.ft_consumidores\")\n",
    "\n",
    "#aplicar as transformações\n",
    "df_consumidores_silver = df_consumidores_bronze \\\n",
    "    .select(\n",
    "        F.col(\"customer_id\").alias(\"id_consumidor\"),\n",
    "        F.col(\"customer_zip_code_prefix\").alias(\"prefixo_cep\"),\n",
    "        F.upper(F.col(\"customer_city\")).alias(\"cidade\"),  \n",
    "        F.upper(F.col(\"customer_state\")).alias(\"estado\")  \n",
    "    ) \\\n",
    "    .dropDuplicates([\"id_consumidor\"]) #regra: id_consumidor não deve conter valores duplicados\n",
    "\n",
    "#salvar na camada silver\n",
    "df_consumidores_silver.write.mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"silver.ft_consumidores\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbd49148-7283-448d-9890-97163f2885ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#silver.ft_pedidos\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import TimestampType\n",
    "from itertools import chain \n",
    "print(\"Processando silver.ft_pedidos...\")\n",
    "\n",
    "#ler a tabela bronze\n",
    "df_pedidos_bronze = spark.table(\"bronze.ft_pedidos\")\n",
    "\n",
    "#mapeamento do Status\n",
    "mapeamento_status = {\n",
    "    'delivered': 'entregue', 'invoiced': 'faturado', 'shipped': 'enviado',\n",
    "    'processing': 'em processamento', 'unavailable': 'indisponível',\n",
    "    'canceled': 'cancelado', 'created': 'criado', 'approved': 'aprovado'\n",
    "}\n",
    "\n",
    "map_expr = F.create_map([F.lit(x) for x in chain(*mapeamento_status.items())])\n",
    "\n",
    "#aplicar as transformações\n",
    "df_pedidos_silver = df_pedidos_bronze \\\n",
    "    .withColumn(\"status\", map_expr[F.col(\"order_status\")]) \\\n",
    "    .withColumn(\"pedido_compra_timestamp\", F.col(\"order_purchase_timestamp\").cast(TimestampType())) \\\n",
    "    .withColumn(\"pedido_entregue_timestamp\", F.col(\"order_delivered_customer_date\").cast(TimestampType())) \\\n",
    "    .withColumn(\"pedido_estimativa_entrega_timestamp\", F.col(\"order_estimated_delivery_date\").cast(TimestampType())) \\\n",
    "    .withColumn(\"tempo_entrega_dias\", F.datediff(F.col(\"pedido_entregue_timestamp\"), F.col(\"pedido_compra_timestamp\"))) \\\n",
    "    .withColumn(\"tempo_entrega_estimado_dias\", F.datediff(F.col(\"pedido_estimativa_entrega_timestamp\"), F.col(\"pedido_compra_timestamp\"))) \\\n",
    "    .withColumn(\"diferenca_entrega_dias\", \n",
    "        F.col(\"tempo_entrega_dias\") - F.col(\"tempo_entrega_estimado_dias\")\n",
    "    ) \\\n",
    "    .withColumn(\"entrega_no_prazo\",\n",
    "        F.when(F.col(\"pedido_entregue_timestamp\").isNull(), \"Não Entregue\")\n",
    "         .when(F.col(\"diferenca_entrega_dias\") <= 0, \"Sim\")\n",
    "         .otherwise(\"Não\")\n",
    "    ) \\\n",
    "    .select(\n",
    "        F.col(\"order_id\").alias(\"id_pedido\"),\n",
    "        F.col(\"customer_id\").alias(\"id_consumidor\"),\n",
    "        \"status\",\n",
    "        \"pedido_compra_timestamp\",\n",
    "        F.col(\"order_approved_at\").alias(\"pedido_aprovado_timestamp\").cast(TimestampType()),\n",
    "        F.col(\"order_delivered_carrier_date\").alias(\"pedido_carregado_timestamp\").cast(TimestampType()),\n",
    "        \"pedido_entregue_timestamp\",\n",
    "        \"pedido_estimativa_entrega_timestamp\",\n",
    "        \"tempo_entrega_dias\",\n",
    "        \"tempo_entrega_estimado_dias\",\n",
    "        \"diferenca_entrega_dias\",\n",
    "        \"entrega_no_prazo\"\n",
    "    )\n",
    "\n",
    "#salvar na camada silver\n",
    "df_pedidos_silver.write.mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"silver.ft_pedidos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81fb24d5-b663-480e-97fe-67ae21c5a837",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#ft_itenspedidos\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DecimalType, IntegerType\n",
    "\n",
    "print(\"Processando silver.ft_itens_pedidos...\")\n",
    "\n",
    "df_itens_bronze = spark.table(\"bronze.ft_itens_pedidos\")\n",
    "\n",
    "df_itens_silver = df_itens_bronze.select(\n",
    "    F.col(\"order_id\").alias(\"id_pedido\"),\n",
    "    F.col(\"order_item_id\").alias(\"id_item\").cast(IntegerType()),\n",
    "    F.col(\"product_id\").alias(\"id_produto\"),\n",
    "    F.col(\"seller_id\").alias(\"id_vendedor\"),\n",
    "    F.col(\"price\").alias(\"preco_BRL\").cast(DecimalType(12, 2)),\n",
    "    F.col(\"freight_value\").alias(\"preco_frete\").cast(DecimalType(12, 2))\n",
    ")\n",
    "\n",
    "df_itens_silver.write.mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"silver.ft_itens_pedidos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c68f21b-9c0b-4396-88fb-2dfdccf772dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#ft_pagamentos pedido\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DecimalType, IntegerType\n",
    "from itertools import chain\n",
    "\n",
    "print(\"Processando silver.ft_pagamentos_pedidos...\")\n",
    "\n",
    "df_pagamentos_bronze = spark.table(\"bronze.ft_pagamentos_pedidos\")\n",
    "\n",
    "mapeamento_pagamento = {\n",
    "    'credit_card': 'Cartão de Crédito',\n",
    "    'boleto': 'Boleto',\n",
    "    'voucher': 'Voucher',\n",
    "    'debit_card': 'Cartão de Débito'\n",
    "}\n",
    "map_expr = F.create_map([F.lit(x) for x in chain(*mapeamento_pagamento.items())])\n",
    "\n",
    "df_pagamentos_silver = df_pagamentos_bronze.select(\n",
    "    F.col(\"order_id\").alias(\"id_pedido\"),\n",
    "    F.col(\"payment_sequential\").alias(\"codigo_pagamento\").cast(IntegerType()),\n",
    "    \n",
    "    F.when(map_expr.isNull(), \"Outro\")\n",
    "     .otherwise(map_expr[F.col(\"payment_type\")])\n",
    "     .alias(\"forma_pagamento\"),\n",
    "     \n",
    "    F.col(\"payment_installments\").alias(\"parcelas\").cast(IntegerType()),\n",
    "    F.col(\"payment_value\").alias(\"valor_pagamento\").cast(DecimalType(12, 2))\n",
    ")\n",
    "\n",
    "df_pagamentos_silver.write.mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"silver.ft_pagamentos_pedidos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8fd4b1d-1240-4443-9419-d13312043042",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#ft_avaliacoes_pedidos\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "print(\"Iniciando 'solucao_avaliacoes'...\")\n",
    "\n",
    "#ler tabela Bronze\n",
    "df_avaliacoes_bronze = spark.table(\"bronze.ft_avaliacoes_pedidos\")\n",
    "\n",
    "#contagem inicial\n",
    "count_antes = df_avaliacoes_bronze.count()\n",
    "\n",
    "#conversão de datas com tolerância a erro (NULL se inválida)\n",
    "df_com_datas_tratadas = df_avaliacoes_bronze \\\n",
    "    .withColumn(\"data_comentario_cast\", F.expr(\"try_to_timestamp(review_creation_date)\")) \\\n",
    "    .withColumn(\"data_resposta_cast\", F.expr(\"try_to_timestamp(review_answer_timestamp)\"))\n",
    "\n",
    "\n",
    "df_avaliacoes_limpo = df_com_datas_tratadas \\\n",
    "    .withColumn(\"avaliacao_cast\", F.expr(\"try_cast(review_score as int)\")) \\\n",
    "    .filter(\n",
    "        (F.col(\"order_id\").isNotNull()) &\n",
    "        (F.col(\"data_comentario_cast\").isNotNull()) &\n",
    "        (F.col(\"data_resposta_cast\").isNotNull()) &\n",
    "        (F.col(\"avaliacao_cast\").between(1, 5)) &\n",
    "        (F.col(\"data_comentario_cast\") <= F.current_timestamp()) &\n",
    "        (F.col(\"data_comentario_cast\") >= F.lit(\"2010-01-01\"))\n",
    "    )\n",
    "\n",
    "#contagem final e relatório\n",
    "count_depois = df_avaliacoes_limpo.count()\n",
    "linhas_removidas = count_antes - count_depois\n",
    "\n",
    "print(\"--- Relatório de Limpeza ---\")\n",
    "print(f\"Linhas antes: {count_antes}\")\n",
    "print(f\"Linhas removidas (IDs, datas ou scores inválidos): {linhas_removidas}\")\n",
    "print(f\"Linhas depois: {count_depois}\")\n",
    "\n",
    "#selecionar e renomear colunas conforme mapeamento\n",
    "df_avaliacoes_silver = df_avaliacoes_limpo.select(\n",
    "    F.col(\"review_id\").alias(\"id_avaliacao\"),\n",
    "    F.col(\"order_id\").alias(\"id_pedido\"),\n",
    "    F.col(\"avaliacao_cast\").alias(\"avaliacao\"),\n",
    "    F.col(\"review_comment_title\").alias(\"titulo_comentario\"),\n",
    "    F.col(\"review_comment_message\").alias(\"comentario\"),\n",
    "    F.col(\"data_comentario_cast\").alias(\"data_comentario\"),\n",
    "    F.col(\"data_resposta_cast\").alias(\"data_resposta\")\n",
    ")\n",
    "\n",
    "#escreve em silver\n",
    "df_avaliacoes_silver.write.mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"silver.ft_avaliacoes_pedidos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "baaadb89-ba86-4253-b44f-1d5fe8f3b097",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#ft_produtos\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "print(\"Processando silver.ft_produtos...\")\n",
    "\n",
    "df_produtos_bronze = spark.table(\"bronze.ft_produtos\")\n",
    "\n",
    "df_produtos_silver = df_produtos_bronze.select(\n",
    "    F.col(\"product_id\").alias(\"id_produto\"),\n",
    "    F.col(\"product_category_name\").alias(\"categoria_produto\"),\n",
    "    F.col(\"product_weight_g\").alias(\"peso_produto_gramas\").cast(IntegerType()),\n",
    "    F.col(\"product_length_cm\").alias(\"comprimento_centimetros\").cast(IntegerType()),\n",
    "    F.col(\"product_height_cm\").alias(\"altura_centimetros\").cast(IntegerType()),\n",
    "    F.col(\"product_width_cm\").alias(\"largura_centimetros\").cast(IntegerType())\n",
    ")\n",
    "\n",
    "df_produtos_silver.write.mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"silver.ft_produtos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7390bdbc-cebf-4dcf-8608-8eecac81a8b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#ft_vendedores\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "print(\"Processando silver.ft_vendedores...\")\n",
    "\n",
    "df_vendedores_bronze = spark.table(\"bronze.ft_vendedores\")\n",
    "\n",
    "df_vendedores_silver = df_vendedores_bronze.select(\n",
    "    F.col(\"seller_id\").alias(\"id_vendedor\"),\n",
    "    F.col(\"seller_zip_code_prefix\").alias(\"prefixo_cep\").cast(IntegerType()),\n",
    "    F.upper(F.col(\"seller_city\")).alias(\"cidade\"),\n",
    "    F.upper(F.col(\"seller_state\")).alias(\"estado\")\n",
    ")\n",
    "\n",
    "df_vendedores_silver.write.mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"silver.ft_vendedores\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa643e61-bdb2-489b-abac-7c5c671e3362",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#ft_categoria_produto_traducao\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "print(\"Processando silver.dm_categoria_produtos_traducao...\")\n",
    "\n",
    "df_traducao_bronze = spark.table(\"bronze.dm_categoria_produtos_traducao\")\n",
    "\n",
    "df_traducao_silver = df_traducao_bronze.select(\n",
    "    F.col(\"product_category_name\").alias(\"nome_produto_pt\"),\n",
    "    F.col(\"product_category_name_english\").alias(\"nome_produto_en\")\n",
    ")\n",
    "\n",
    "df_traducao_silver.write.mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"silver.dm_categoria_produtos_traducao\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3712a617-1704-4936-b9dd-8dacdc6c52ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#ft_cotacao\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DecimalType, DateType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "print(\"Processando silver.dm_cotacao_dolar...\")\n",
    "\n",
    "df_cotacao_bronze = spark.table(\"bronze.dm_cotacao_dolar\") \\\n",
    "    .select(\n",
    "        F.to_date(F.col(\"dataHoraCotacao\")).alias(\"data\"),\n",
    "        F.col(\"cotacaoCompra\").cast(DecimalType(12, 2)).alias(\"cotacao_dolar\")\n",
    "    ) \\\n",
    "    .dropDuplicates([\"data\"]) \\\n",
    "    .orderBy(\"data\")\n",
    "\n",
    "min_max_data = df_cotacao_bronze.select(\n",
    "    F.min(\"data\").alias(\"data_min\"),\n",
    "    F.max(\"data\").alias(\"data_max\")\n",
    ").first()\n",
    "\n",
    "df_calendario = spark.sql(f\"SELECT explode(sequence(to_date('{min_max_data.data_min}'), to_date('{min_max_data.data_max}'), interval 1 day)) AS data\")\n",
    "\n",
    "df_cotacao_com_nulos = df_calendario.join(\n",
    "    df_cotacao_bronze,\n",
    "    on=\"data\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "window_spec = Window.orderBy(\"data\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "df_cotacao_preenchida = df_cotacao_com_nulos.withColumn(\n",
    "    \"cotacao_preenchida\",\n",
    "    F.last(F.col(\"cotacao_dolar\"), ignorenulls=True).over(window_spec)\n",
    ")\n",
    "\n",
    "df_cotacao_silver = df_cotacao_preenchida.select(\n",
    "    F.col(\"data\"),\n",
    "    F.col(\"cotacao_preenchida\").alias(\"cotacao_dolar\")\n",
    ") \\\n",
    ".filter(F.col(\"cotacao_dolar\").isNotNull()) \n",
    "\n",
    "df_cotacao_silver.write.mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"silver.dm_cotacao_dolar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "379a1148-e588-42be-80ae-deb825d32c7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "print(\"Iniciando Validações de Integridade (Remoção de Órfãos)...\")\n",
    "\n",
    "#carregar as tabelas silver que acabamos de criar ---\n",
    "df_pedidos = spark.table(\"silver.ft_pedidos\")\n",
    "df_consumidores = spark.table(\"silver.ft_consumidores\")\n",
    "df_itens = spark.table(\"silver.ft_itens_pedidos\")\n",
    "\n",
    "#verificação de Pedidos Órfãos (sem consumidor) ---\n",
    "\n",
    "#encontra pedidos cujo 'id_consumidor' NÃO existe na tabela de consumidores\n",
    "df_pedidos_orfãos = df_pedidos.join(\n",
    "    df_consumidores,\n",
    "    on=\"id_consumidor\",\n",
    "    how=\"left_anti\" # 'left_anti' retorna apenas linhas de 'df_pedidos' que NÃO têm correspondência\n",
    ")\n",
    "\n",
    "count_pedidos_orfãos = df_pedidos_orfãos.count()\n",
    "print(f\"Relatório de Validação 1: Pedidos Órfãos (sem consumidor) encontrados: {count_pedidos_orfãos}\")\n",
    "\n",
    "\n",
    "df_itens_orfãos = df_itens.join(\n",
    "    df_pedidos,\n",
    "    on=\"id_pedido\",\n",
    "    how=\"left_anti\"\n",
    ")\n",
    "\n",
    "count_itens_orfãos = df_itens_orfãos.count()\n",
    "print(f\"Relatório de Validação 2: Itens Órfãos (sem pedido) encontrados: {count_itens_orfãos}\")\n",
    "\n",
    "#remover os registros órfãos e sobrescrever as tabelas\n",
    "\n",
    "#se encontramos órfãos, limpamos a tabela de pedidos\n",
    "if count_pedidos_orfãos > 0:\n",
    "    print(f\"Limpando {count_pedidos_orfãos} pedidos órfãos da tabela silver.ft_pedidos...\")\n",
    "    # lef semi faz o oposto: mantém apenas os pedidos QUE TÊM correspondência\n",
    "    df_pedidos_limpos = df_pedidos.join(\n",
    "        df_consumidores,\n",
    "        on=\"id_consumidor\",\n",
    "        how=\"left_semi\" \n",
    "    )\n",
    "    #sobrescreve a tabela SÓ COM OS PEDIDOS VÁLIDOS\n",
    "    df_pedidos_limpos.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"silver.ft_pedidos\")\n",
    "else:\n",
    "    print(\"Nenhum pedido órfão para limpar.\")\n",
    "\n",
    "\n",
    "#se encontramos órfãos limpamos a tabela de itens\n",
    "if count_itens_orfãos > 0:\n",
    "    print(f\"Limpando {count_itens_orfãos} itens órfãos da tabela silver.ft_itens_pedidos...\")\n",
    "    #mantém apenas os itens QUE TÊM um pedido válido\n",
    "    df_itens_limpos = df_itens.join(\n",
    "        df_pedidos,\n",
    "        on=\"id_pedido\",\n",
    "        how=\"left_semi\"\n",
    "    )\n",
    "    #sobrescreve a tabela SÓ COM OS ITENS VÁLIDOS\n",
    "    df_itens_limpos.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"silver.ft_itens_pedidos\")\n",
    "else:\n",
    "    print(\"Nenhum item órfão para limpar.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "450e0915-5f13-4d52-b4fa-f33e32903977",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#pedido_total\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DecimalType\n",
    "\n",
    "print(\"Iniciando a criação da tabela final silver.pedido_total...\")\n",
    "\n",
    "#ler as table silver\n",
    "df_pedidos = spark.table(\"silver.ft_pedidos\")\n",
    "df_pagamentos = spark.table(\"silver.ft_pagamentos_pedidos\")\n",
    "df_cotacao = spark.table(\"silver.dm_cotacao_dolar\")\n",
    "\n",
    "df_pagamentos_total = df_pagamentos \\\n",
    "    .groupBy(\"id_pedido\") \\\n",
    "    .agg(\n",
    "        F.sum(\"valor_pagamento\").alias(\"valor_total_pago_brl\")\n",
    "    )\n",
    "\n",
    "#juntar_pedidos_e_pagamentos\n",
    "df_pedidos_com_pagamento = df_pedidos.join(\n",
    "    df_pagamentos_total,\n",
    "    on=\"id_pedido\",\n",
    "    how=\"left\" #manter_pedidos_mesmo_sem_pagamento\n",
    ")\n",
    "\n",
    "#join pela data\n",
    "df_pedidos_pronto = df_pedidos_com_pagamento.withColumn(\n",
    "    \"data_pedido\", F.to_date(F.col(\"pedido_compra_timestamp\"))\n",
    ")\n",
    "\n",
    "#juntar com cotacao do dolar\n",
    "df_final = df_pedidos_pronto.join(\n",
    "    df_cotacao,\n",
    "    df_pedidos_pronto.data_pedido == df_cotacao.data, #join_pela_data\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "#calcular valor usd \n",
    "df_pedido_total_silver = df_final.select(\n",
    "    F.col(\"id_pedido\"),\n",
    "    F.col(\"id_consumidor\"),\n",
    "    F.col(\"status\"),\n",
    "    F.col(\"valor_total_pago_brl\").cast(DecimalType(12, 2)),\n",
    "    \n",
    " \n",
    "    (F.col(\"valor_total_pago_brl\") / F.col(\"cotacao_dolar\"))\n",
    "        .alias(\"valor_total_pago_usd\").cast(DecimalType(12, 2)),\n",
    "        \n",
    "    F.col(\"data_pedido\")\n",
    ") \\\n",
    ".filter(F.col(\"valor_total_pago_brl\").isNotNull()) \n",
    "\n",
    "#salvar em silver\n",
    "df_pedido_total_silver.write.mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"silver.pedido_total\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "silverlayer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
